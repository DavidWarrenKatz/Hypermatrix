{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f247524",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# imports\n",
    "############################################\n",
    "\n",
    "import pyBigWig\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from heapq import nlargest\n",
    "import copy\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "from scipy.stats import pearsonr\n",
    "from config_and_print import methy_directory, filtered_list, chrom_file, resolutions, output_directory, mappability_threshold, normalization\n",
    "#chromosomes = [f'chr{chrom}' for chrom in chromosomes]\n",
    "\n",
    "# Ensure resolutions is treated as a tuple or list of strings\n",
    "if isinstance(resolutions, str):\n",
    "    resolutions = (resolutions,)\n",
    "\n",
    "# Print resolutions for debugging\n",
    "print(f\"Resolutions from config: {resolutions}\")\n",
    "\n",
    "# Extract resolution value and label from the resolutions string\n",
    "resolution_str = resolutions[0]\n",
    "\n",
    "# Debug print to check the value of resolution_str\n",
    "print(f\"Extracted resolution string: {resolution_str}\")\n",
    "\n",
    "def parse_resolution(resolution_str):\n",
    "    if ':' in resolution_str:\n",
    "        resolution_value, resolution_label = resolution_str.split(':')\n",
    "        try:\n",
    "            resolution = int(resolution_value)\n",
    "            return resolution, resolution_label\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Resolution value should be an integer: '{resolution_value}' in '{resolution_str}'\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid resolution format: '{resolution_str}'. Expected format 'value:label', e.g., '1000000:1Mb'.\")\n",
    "\n",
    "resolution, resolution_label = parse_resolution(resolution_str)\n",
    "\n",
    "########################################################################\n",
    "# create the cell type dictionary\n",
    "# [TO DO] This needs to be replaced with SNPS code \n",
    "########################################################################\n",
    "# Define the path file with prefixes and colors in the following form\n",
    "#1       sc1.ACTTGA      red\n",
    "#2       sc1.GCCAAT      red\n",
    "#3       sc1.TAGCTT      red\n",
    "#4       sc10.TAGCTT     blue\n",
    "#\n",
    "filename = '../../bin/name.order.HCG_methy.with_color.txt'\n",
    "\n",
    "# Initialize an empty dictionary to store cell ID and color\n",
    "cell_color_dict = {}\n",
    "\n",
    "# Open and read the file\n",
    "with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into parts\n",
    "        parts = line.strip().split()\n",
    "        # Extract cell ID and color\n",
    "        cell_id = parts[1]\n",
    "        color = parts[2]\n",
    "        # Store in dictionary\n",
    "        cell_color_dict[cell_id] = color\n",
    "\n",
    "# Define the path to the tensor sample order file\n",
    "#This file contains the prefixes in the form\n",
    "#sc11.ACTTGA\n",
    "#sc11.CGATGT\n",
    "#sc11.GCCAAT\n",
    "#\n",
    "tensor_order_filename = f'{output_directory}/filtered_bam_list.txt'\n",
    "\n",
    "# Initialize a list to store the 1s and 0s\n",
    "color_vector = []\n",
    "\n",
    "# Open and read the tensor sample order file\n",
    "with open(tensor_order_filename, 'r') as file:\n",
    "    for line in file:\n",
    "        sample_id = line.strip()  # Remove any trailing newlines or spaces\n",
    "        if sample_id in cell_color_dict and cell_color_dict[sample_id] == 'red':\n",
    "            color_vector.append(1)\n",
    "        else:\n",
    "            color_vector.append(0)\n",
    "\n",
    "# Output the color vector to check\n",
    "print(len(color_vector))\n",
    "\n",
    "# Create a mapping dictionary\n",
    "color_mapping = {\n",
    "    'red': 'imr90',\n",
    "    'blue': 'gm12878'\n",
    "}\n",
    "\n",
    "# Update the dictionary using the mapping\n",
    "updated_cell_color_dict = {key: color_mapping[value] for key, value in cell_color_dict.items()}\n",
    "\n",
    "##############################################################################\n",
    "#import bins to remove created previously\n",
    "#############################################################################\n",
    "\n",
    "#Import bins to remove dictionary\n",
    "bins_file_path = f'{output_directory}/bins_to_remove_res{resolution_label}.npz'\n",
    "if os.path.exists(bins_file_path):\n",
    "    print(f\"{bins_file_path} exist. Importing.\")\n",
    "    #load the dark regions data and the A/B compartment data\n",
    "    loaded_data = np.load(bins_file_path)\n",
    "    # Convert the loaded data back into a dictionary with the same structure\n",
    "    bins_to_remove = {chrom: loaded_data[chrom] for chrom in loaded_data}\n",
    "\n",
    "###########################################################################    \n",
    "#create a dictionary of the A/B compartment calls for the bulk data\n",
    "###########################################################################\n",
    "bulk_data = {}\n",
    "path_to_eigenvectors = '../../projects/single_cell_files/eigenvector/'\n",
    "\n",
    "for i in range(1, 23):\n",
    "    file = path_to_eigenvectors + f'res{resolution}_ch{i}_oe_GM12878_{normalization}_eigenvector.txt'\n",
    "    key = os.path.splitext(os.path.basename(file))[0]  \n",
    "    bulk_data[key] = pd.read_csv(file, header=None, names=['eigenvalue'])\n",
    "    file = path_to_eigenvectors + f'res{resolution}_ch{i}_oe_IMR90_{normalization}_eigenvector.txt'\n",
    "    key = os.path.splitext(os.path.basename(file))[0]  \n",
    "    bulk_data[key] = pd.read_csv(file, header=None, names=['eigenvalue'])\n",
    "    \n",
    "###########################################################################\n",
    "# download H3K9ac file if it does not exist\n",
    "###########################################################################\n",
    "\n",
    "def calculate_bin_averages(data, elements_per_bin):\n",
    "    num_bins = math.ceil(len(data)/elements_per_bin)\n",
    "    bin_averages = np.zeros(num_bins)\n",
    "    for i in range(num_bins):\n",
    "        start_index = i * elements_per_bin\n",
    "        end_index = min((i + 1) * elements_per_bin, len(data))\n",
    "        bin_data = data[start_index:end_index]\n",
    "        if len(bin_data) > 0:\n",
    "            bin_averages[i] = np.mean(bin_data)\n",
    "        else:\n",
    "            bin_averages[i] = 0\n",
    "    return np.nan_to_num(bin_averages, nan=0.0)\n",
    "\n",
    "# Read chromosome sizes from hg19.autosome.chrom.sizes\n",
    "lengths = {}\n",
    "with open(chrom_file, 'r') as file:\n",
    "    for line in file:\n",
    "        chrom, size = line.strip().split()\n",
    "        lengths[chrom] = int(size)\n",
    "\n",
    "# Check if the output file already exists\n",
    "output_file = f'../../bin/softwarefiles/h3k9ac_res{resolution}_GM12878.pkl'\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    # Initialization\n",
    "    chromosomes = list(lengths.keys())\n",
    "    h3k9ac = {name: [] for name in chromosomes}\n",
    "\n",
    "    bigwig_H3K9ac_path = '../../bin/softwarefiles/ENCFF128UVW_hg19_H3K9ac_GM12878.bigWig'\n",
    "    if not os.path.exists(bigwig_H3K9ac_path):\n",
    "        os.system(f'wget https://www.encodeproject.org/files/ENCFF128UVW/@@download/ENCFF128UVW.bigWig -O {bigwig_H3K9ac_path}')\n",
    "\n",
    "    bigwig_H3K9ac = pyBigWig.open(bigwig_H3K9ac_path)\n",
    "    \n",
    "    # Process each chromosome\n",
    "    for chromosome_name in chromosomes:\n",
    "        start_position = 1\n",
    "        end_position = lengths[chromosome_name]\n",
    "        values_file1 = np.array(bigwig_H3K9ac.values(chromosome_name, start_position, end_position))\n",
    "        h3k9ac[chromosome_name] = calculate_bin_averages(values_file1, resolution)\n",
    "\n",
    "    # Save the results in a pickle file\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(h3k9ac, file)\n",
    "else:\n",
    "    print(f\"The file {output_file} already exists.\")\n",
    "    with open(output_file, 'rb') as file:\n",
    "        h3k9ac = pickle.load(file)\n",
    "\n",
    "################################################################################    \n",
    "#make sure each GM12878 eigenvector has positive value for active A compartment\n",
    "################################################################################\n",
    "for i in range(1, 23):\n",
    "    key_gm12878 = f'res{resolution}_ch{i}_oe_GM12878_{normalization}_eigenvector'\n",
    "    chromosome_key = f'chr{i}'\n",
    "    h3k9ac_df = pd.DataFrame(h3k9ac[chromosome_key], columns=['H3K9ac_signal'])\n",
    "    \n",
    "    df_gm12878_positive = bulk_data[key_gm12878]['eigenvalue']  \n",
    "    df_gm12878_negative = -bulk_data[key_gm12878]['eigenvalue']\n",
    "\n",
    "    # Compute correlations by first ensuring eigenvector data is in DataFrame format\n",
    "    corr_positive = df_gm12878_positive.corr(h3k9ac_df['H3K9ac_signal'])\n",
    "    corr_negative = df_gm12878_negative.corr(h3k9ac_df['H3K9ac_signal'])\n",
    "    if corr_negative > corr_positive:\n",
    "        bulk_data[key_gm12878]['eigenvalue'] = -bulk_data[key_gm12878]['eigenvalue']\n",
    "        print(f\"Switched for chromosome {i}\")    \n",
    "\n",
    "###############################################################################    \n",
    "#make sure each IMR90 eigenvector has consistent orientation with GM12878\n",
    "################################################################################\n",
    "for i in range(1, 23):\n",
    "    # Construct keys for GM12878 and IMR90\n",
    "    key_gm12878 = f'res{resolution}_ch{i}_oe_GM12878_{normalization}_eigenvector'\n",
    "    key_imr90 = f'res{resolution}_ch{i}_oe_GM12878_{normalization}_eigenvector'\n",
    "    \n",
    "    # Retrieve DataFrames for GM12878 and IMR90\n",
    "    df_gm12878 = bulk_data[key_gm12878]\n",
    "    df_imr90 = bulk_data[key_imr90]\n",
    "    \n",
    "    # Ensure data is in expected format\n",
    "    if not df_gm12878.empty and not df_imr90.empty:\n",
    "        # Concatenate DataFrames side-by-side\n",
    "        combined_df = pd.concat([df_gm12878.reset_index(drop=True), df_imr90.reset_index(drop=True)], axis=1, keys=['gm12878', 'imr90'])\n",
    "        \n",
    "        # Drop rows with NaN values in either column\n",
    "        combined_df.dropna(inplace=True)\n",
    "        \n",
    "        # Extract Series after dropping NaNs\n",
    "        gm12878_series = combined_df['gm12878']['eigenvalue']\n",
    "        imr90_series = combined_df['imr90']['eigenvalue']\n",
    "\n",
    "        # Calculate correlations\n",
    "        corr_positive = gm12878_series.corr(imr90_series)\n",
    "        corr_negative = gm12878_series.corr(-imr90_series)\n",
    "\n",
    "        # If negating IMR90 improves correlation, update the original data\n",
    "        if corr_negative > corr_positive:\n",
    "            bulk_data[key_imr90]['eigenvalue'] = -bulk_data[key_imr90]['eigenvalue']\n",
    "            print(f\"switched for chromosome {i}\")\n",
    "\n",
    "#The A/B compartments of the proper orientation before dropping dark regions\n",
    "original_bulk_data = copy.deepcopy(bulk_data)\n",
    "\n",
    "#################################################################################\n",
    "#remove dark regions\n",
    "#dark reigons are obviously correlated\n",
    "#I want to remove dark regions to get meeaningfully correlated regions\n",
    "###############################################################################\n",
    "\n",
    "for i in range(1, 23):\n",
    "    # Construct the keys for GM12878 and IMR90\n",
    "    key_gm12878 = f'res{resolution}_ch{i}_oe_GM12878_{normalization}_eigenvector'\n",
    "    key_imr90 = f'res{resolution}_ch{i}_oe_IMR90_{normalization}_eigenvector'\n",
    "    chrom = f'chr{i}'\n",
    "\n",
    "    # Check if the chromosome exists in the bins_to_remove and in the data\n",
    "    if chrom in bins_to_remove and key_gm12878 in bulk_data and key_imr90 in bulk_data:\n",
    "        # Retrieve the indices to remove for this chromosome\n",
    "        indices_to_remove = bins_to_remove[chrom]\n",
    "\n",
    "        # Initialize lists to hold valid indices for both DataFrames\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx in indices_to_remove:\n",
    "            if idx < len(bulk_data[key_gm12878]) and idx < len(bulk_data[key_imr90]):\n",
    "                valid_indices.append(idx)\n",
    "        \n",
    "        # Now drop the valid indices from both DataFrames\n",
    "        if valid_indices:\n",
    "            bulk_data[key_gm12878] = bulk_data[key_gm12878].drop(valid_indices).reset_index(drop=True)\n",
    "            bulk_data[key_imr90] = bulk_data[key_imr90].drop(valid_indices).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# This is where AB_calls are saved\n",
    "\n",
    "def extract_prefixes(file_path):\n",
    "    \"\"\"Extract prefixes from the filtered_bam_list.txt file.\"\"\"\n",
    "    prefixes = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Assuming the prefix format is like \"sc11.ACTTGA\" or similar\n",
    "            parts = line.strip().split('.')\n",
    "            if len(parts) >= 2:\n",
    "                prefix = parts[0] + '.' + parts[1]  # Concatenate the first two parts to get the full prefix\n",
    "                prefixes.append(prefix)\n",
    "    return prefixes\n",
    "\n",
    "def load_h5_file(file_path, dataset_name):\n",
    "    \"\"\"Load a dataset from an HDF5 file.\"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        data = f[dataset_name][:]\n",
    "    return data\n",
    "\n",
    "def get_best_correlated_vector(V, eigenvector):\n",
    "    \"\"\"Find the row in V that best correlates with the given eigenvector.\"\"\"\n",
    "    eigenvector = eigenvector.values.flatten()\n",
    "    best_corr = -np.inf  # Initialize with a very low correlation\n",
    "    best_index = -1\n",
    "    best_vector = None\n",
    "\n",
    "    # Check each row in V for correlation with the eigenvector\n",
    "    for i in range(V.shape[0]):  # Iterate over all rows in V\n",
    "        # Synchronize non-NaN data\n",
    "        valid_indices = ~np.isnan(V[i, :]) & ~np.isnan(eigenvector)\n",
    "        if np.any(valid_indices):\n",
    "            corr, _ = pearsonr(V[i, valid_indices], eigenvector[valid_indices])\n",
    "            if corr > best_corr:  # Check if this is the best correlation so far\n",
    "                best_corr = corr\n",
    "                best_index = i\n",
    "                best_vector = V[i, :]\n",
    "\n",
    "    return best_vector, best_index, best_corr\n",
    "\n",
    "# Directory setup\n",
    "output_directory = '../../projects/single_cell_files/'\n",
    "filtered_bam_list = os.path.join(output_directory, 'filtered_bam_list.txt')\n",
    "base_tensor_dir = os.path.join(output_directory, 'tensor_1Mb_AB_factors')\n",
    "\n",
    "# Extract prefixes from the file\n",
    "prefixes = extract_prefixes(filtered_bam_list)\n",
    "print(f\"Extracted prefixes: {prefixes}\")\n",
    "\n",
    "# Dictionary to hold the dataframes for each chromosome\n",
    "chromosome_results = {}\n",
    "\n",
    "def normalize_vectors(V):\n",
    "    \"\"\"Normalize the columns of V to have a norm of 1.\"\"\"\n",
    "    norms = np.linalg.norm(V, axis=0)\n",
    "    norms[norms == 0] = 1  # Prevent division by zero\n",
    "    V_normalized = V / norms\n",
    "    return .02 * V_normalized - .01\n",
    "\n",
    "# Function to replace specific bins with NaNs, ensuring the bin is valid\n",
    "def replace_bins_with_nans(compartment_values, bins_to_remove):\n",
    "    compartment_values = np.array(compartment_values)  # Convert to numpy array if not already\n",
    "    valid_bins_to_remove = [bin_idx for bin_idx in bins_to_remove if bin_idx < len(compartment_values)]\n",
    "    compartment_values[valid_bins_to_remove] = np.nan  # Replace values in valid bins to remove with NaNs\n",
    "    return compartment_values\n",
    "\n",
    "# Loop through each chromosome directory\n",
    "for i in range(1, 23):\n",
    "    chromosome = f'chr{i}'\n",
    "    results_df = pd.DataFrame(columns=['Sample', 'A/B Compartment', 'Correlation With Bulk', 'Cell Type'])\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        input_file = os.path.join(base_tensor_dir, chromosome, f'{prefix}_compartments.h5')\n",
    "        output_dir = os.path.join(output_directory, 'tensor_1Mb_AB_calls', chromosome)\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        output_file = os.path.join(output_dir, f'{prefix}_tensor_AB_compartment_call.h5')\n",
    "        text_output_file = os.path.join(output_dir, f'{prefix}_tensor_AB_compartment_call.txt')\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"{output_file} already exists. Loading existing results.\")\n",
    "            with h5py.File(output_file, 'r') as output_h5:\n",
    "                best_vector = output_h5['AB_Compartment'][:]\n",
    "                best_corr = output_h5['correlation']\n",
    "                cell_type = output_h5['cell_type']\n",
    " \n",
    "        else:\n",
    "            sample_id = prefix  \n",
    "            cell_type = updated_cell_color_dict.get(sample_id, 'GM12878')  # Default set to GM12878\n",
    "            key = f'res{resolution}_ch{i}_oe_{cell_type.upper()}_{normalization}_eigenvector'  \n",
    "\n",
    "            print(f\"Processing sample_id: {sample_id}, cell_type: {cell_type}, key: {key}, resolution {resolution}\")\n",
    "\n",
    "            if key in original_bulk_data:\n",
    "                tensor_factors = load_h5_file(input_file, '/compartment_factors')\n",
    "                tensor_factors = normalize_vectors(tensor_factors)\n",
    "                print(f\"Loaded tensor factors from {input_file}, shape: {tensor_factors.shape}\")\n",
    "                bulk_eigenvector = original_bulk_data[key]['eigenvalue'][:-1]\n",
    "                print(f\"Bulk eigenvector shape: {bulk_eigenvector.shape}\")\n",
    "\n",
    "                best_vector, best_index, best_corr = get_best_correlated_vector(tensor_factors, bulk_eigenvector)\n",
    "                print(f\"Best correlation: {best_corr} at index {best_index}\")\n",
    "\n",
    "                # Retrieve the bins to remove for this chromosome\n",
    "                bins_to_remove_for_chrom = bins_to_remove[chromosome]\n",
    "        \n",
    "                # Apply the remove dark bins function to each cell's A/B Compartment data\n",
    "                best_vector = replace_bins_with_nans(best_vector, bins_to_remove_for_chrom)\n",
    "                \n",
    "                # Save the results to the output file (HDF5)\n",
    "                with h5py.File(output_file, 'w') as output_h5:\n",
    "                    output_h5.create_dataset('AB_Compartment', data=best_vector)\n",
    "                    output_h5.create_dataset('correlation', data=best_corr)\n",
    "                    output_h5.create_dataset('cell_type', data=cell_type)\n",
    "\n",
    "                # Save the A/B Compartment values as a text file\n",
    "                np.savetxt(text_output_file, best_vector, fmt='%f')\n",
    "                print(f\"Saved A/B Compartment values to {text_output_file}\")\n",
    "\n",
    "        new_row = pd.DataFrame({\n",
    "            'Sample': [sample_id],\n",
    "            'A/B Compartment': [best_vector],\n",
    "            'Correlation With Bulk': [best_corr],\n",
    "            'Cell Type': [cell_type]\n",
    "        })\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    print(f\"Finished processing {chromosome}, {results_df.shape[0]} rows added.\")\n",
    "    chromosome_results[chromosome] = results_df\n",
    "                                                    \n",
    "# Save original_bulk_data to a file\n",
    "bulk_data_output_file = '../../projects/single_cell_files/original_bulk_data.pkl'\n",
    "with open(bulk_data_output_file, 'wb') as f:\n",
    "    pickle.dump(original_bulk_data, f)\n",
    "print(f\"original_bulk_data saved to {bulk_data_output_file}\")\n",
    "\n",
    "# Save chromosome_results to a file\n",
    "chromosome_results_output_file = '../../projects/single_cell_files/chromosome_results.pkl'\n",
    "with open(chromosome_results_output_file, 'wb') as f:\n",
    "    pickle.dump(chromosome_results, f)\n",
    "print(f\"chromosome_results saved to {chromosome_results_output_file}\")                                                               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiomics6",
   "language": "python",
   "name": "multiomics6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
