{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b728248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwk681/.conda/envs/multiomics6/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-17 22:08:14.534976: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-17 22:08:14.539041: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-17 22:08:14.835586: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-17 22:08:14.845954: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-17 22:08:42.244893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2897, 129)\n",
      "129\n",
      "bam_directory='/home/dwk681/workspace/cluster_cells_from_GSE189158_NOMe_HiC/filesFromCluster/bam'\n",
      "methy_directory='/home/dwk681/workspace/cluster_cells_from_GSE189158_NOMe_HiC/filesFromCluster/bam/methylation/filter_low_qual'\n",
      "software_directory='../../bin/softwarefiles'\n",
      "chrom_file='../../bin/softwarefiles/hg19.autosome.chrom.sizes'\n",
      "fragments_file='../../bin/softwarefiles/hg19_DpnII.txt'\n",
      "output_directory='../../projects/single_cell_files'\n",
      "hg19_fa_url='ftp://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz'\n",
      "filtered_list='../../projects/single_cell_files/filtered_bam_list.txt'\n",
      "schicluster_env='schicluster2'\n",
      "bisulfite_env='bisulfitehic27'\n",
      "min_high_quality_reads='250000'\n",
      "resolutions='1000000:1Mb'\n",
      "impute='True'\n",
      "cluster_compartments='False'\n",
      "cumulant='False'\n",
      "iterations='900'\n",
      "chromosomes=1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22\n",
      "dark_regions_hg19_url='https://www.encodeproject.org/files/ENCFF000EHJ/@@download/ENCFF000EHJ.bigWig'\n",
      "mappability_threshold='0.6'\n",
      "data_type='oe'\n",
      "normalization='NONE'\n",
      "correlation='True'\n",
      "genomeID='hg19'\n",
      "hic_GM12878_url='https://ftp.ncbi.nlm.nih.gov/geo/series/GSE63nnn/GSE63525/suppl/GSE63525%5FGM12878%5Finsitu%5Fprimary%2Breplicate%5Fcombined%5F30%2Ehic'\n",
      "hic_IMR90_url='https://ftp.ncbi.nlm.nih.gov/geo/series/GSE63nnn/GSE63525/suppl/GSE63525%5FIMR90%5Fcombined%5F30.hic'\n",
      "Resolutions from config: ('1000000:1Mb',)\n",
      "Extracted resolution string: 1000000:1Mb\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "# Basic data handling and scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import scipy.stats as stats\n",
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import gzip\n",
    "import math\n",
    "\n",
    "# Machine learning and data analysis tools\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Statistical methods\n",
    "from scipy.stats import zscore, pearsonr, spearmanr\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.interpolate import interpn\n",
    "\n",
    "# Tensor and decomposition\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import non_negative_parafac\n",
    "\n",
    "# Other utilities\n",
    "import pyBigWig\n",
    "from scipy.io import savemat\n",
    "import textwrap\n",
    "\n",
    "# Suppress warnings if necessary\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom or other specific imports\n",
    "import hicstraw  # Assumed specific to your use case\n",
    "\n",
    "#This block imports the 3omics data for 153 single cells of two cell types.\n",
    "\n",
    "base_directory = '/home/dwk681/workspace/hypermatrix_test/hypermatrix/projects/single_cell_files/'\n",
    "\n",
    "def normalize_matrix_columns(A):\n",
    "    \"\"\"\n",
    "    Normalize each column of the matrix so that each has a norm of one.\n",
    "    \n",
    "    Parameters:\n",
    "    - A: a 2D NumPy array (matrix)\n",
    "    \n",
    "    Returns:\n",
    "    - normalized_A: a matrix where each column of A has been divided by its L2 norm\n",
    "    \"\"\"\n",
    "    # Calculate the L2 norm for each column\n",
    "    column_norms = np.linalg.norm(A, axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if np.any(column_norms == 0):\n",
    "        raise ValueError(\"One or more columns have zero norm. Cannot normalize those columns.\")\n",
    "    \n",
    "    # Normalize each column by its norm\n",
    "    normalized_A = A / column_norms\n",
    "    \n",
    "    return normalized_A\n",
    "\n",
    "def calculate_matrix(file_path):\n",
    "    # Initialize an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Read the compressed file using gzip\n",
    "    with gzip.open(file_path, 'rt') as file:\n",
    "        for line in file:\n",
    "            # Split the line into fields\n",
    "            fields = line.strip().split('\\t')\n",
    "\n",
    "            # Extract relevant entries for calculations\n",
    "            numerator_indices = range(6, len(fields), 2)\n",
    "            denominator_indices = range(7, len(fields), 2)\n",
    "\n",
    "            row_data = []\n",
    "            for num_idx, denom_idx in zip(numerator_indices, denominator_indices):\n",
    "                numerator = float(fields[num_idx])\n",
    "                denominator = float(fields[denom_idx])\n",
    "\n",
    "                # Check if denominator is zero, set entry to 0, else perform the division\n",
    "                row_data.append(0 if denominator == 0 else numerator / denominator)\n",
    "\n",
    "            data.append(row_data)\n",
    "\n",
    "    # Convert the list of lists into a NumPy array\n",
    "    data_matrix = np.array(data)\n",
    "\n",
    "    return data_matrix\n",
    "\n",
    "file_path = base_directory + 'b37.autosome.1Mb_interval.add_value.methy.bed.gz'\n",
    "methylation_matrix_1Mb = normalize_matrix_columns(calculate_matrix(file_path))\n",
    "print(methylation_matrix_1Mb.shape)\n",
    "\n",
    "# Define the path to the file\n",
    "file_path = '/home/dwk681/workspace/hypermatrix_test/hypermatrix/projects/single_cell_files/filtered_bam_list.txt'\n",
    "\n",
    "# Initialize an empty list to store the prefixes\n",
    "prefixes = []\n",
    "\n",
    "try:\n",
    "    # Open and read the file with a different encoding\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "        for line in file:\n",
    "            # Strip the newline character and append to the prefixes list\n",
    "            prefixes.append(line.strip())\n",
    "except UnicodeDecodeError:\n",
    "    print(\"Failed to decode file with ISO-8859-1. Trying another method.\")\n",
    "\n",
    "# Print the list of prefixes\n",
    "print(len(prefixes))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from config_and_print import filtered_list, chrom_file, resolutions, normalization\n",
    "\n",
    "output_directory = '../../projects/single_cell_files'\n",
    "\n",
    "# Ensure resolutions is treated as a tuple or list of strings\n",
    "if isinstance(resolutions, str):\n",
    "    resolutions = (resolutions,)\n",
    "\n",
    "print(f\"Resolutions from config: {resolutions}\")\n",
    "resolution_str = resolutions[0]\n",
    "print(f\"Extracted resolution string: {resolution_str}\")\n",
    "\n",
    "def parse_resolution(resolution_str):\n",
    "    if ':' in resolution_str:\n",
    "        resolution_value, resolution_label = resolution_str.split(':')\n",
    "        try:\n",
    "            resolution = int(resolution_value)\n",
    "            return resolution, resolution_label\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Resolution value should be an integer: '{resolution_value}' in '{resolution_str}'\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid resolution format: '{resolution_str}'. Expected format 'value:label', e.g., '1000000:1Mb'.\")\n",
    "\n",
    "resolution, label = parse_resolution(resolution_str)\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "prefix_file_path = '../../projects/single_cell_files/filtered_bam_list.txt'\n",
    "# Read prefixes from the file\n",
    "with open(prefix_file_path, 'r') as f:\n",
    "    prefixes = [line.strip() for line in f] \n",
    "\n",
    "file_list = prefixes\n",
    "\n",
    "chromosomes_info = {\n",
    "    '1': 249250621,\n",
    "    '2': 243199373,\n",
    "    '3': 198022430,\n",
    "    '4': 191154276,\n",
    "    '5': 180915260,\n",
    "    '6': 171115067,\n",
    "    '7': 159138663,\n",
    "    '8': 146364022,\n",
    "    '9': 141213431,\n",
    "    '10': 135534747,\n",
    "    '11': 135006516,\n",
    "    '12': 133851895,\n",
    "    '13': 115169878,\n",
    "    '14': 107349540,\n",
    "    '15': 102531392,\n",
    "    '16': 90354753,\n",
    "    '17': 81195210,\n",
    "    '18': 78077248,\n",
    "    '19': 59128983,\n",
    "    '20': 63025520,\n",
    "    '21': 48129895,\n",
    "    '22': 51304566,\n",
    "}\n",
    "\n",
    "########################################################################\n",
    "# create the cell type dictionary\n",
    "# [TO DO] This needs to be replaced with SNPS code \n",
    "########################################################################\n",
    "# Define the path file with prefixes and colors in the following form\n",
    "#1       sc1.ACTTGA      red\n",
    "#2       sc1.GCCAAT      red\n",
    "#3       sc1.TAGCTT      red\n",
    "#4       sc10.TAGCTT     blue\n",
    "#\n",
    "filename = '../../bin/name.order.HCG_methy.with_color.txt'\n",
    "\n",
    "# Initialize an empty dictionary to store cell ID and color\n",
    "cell_color_dict = {}\n",
    "\n",
    "# Open and read the file\n",
    "with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into parts\n",
    "        parts = line.strip().split()\n",
    "        # Extract cell ID and color\n",
    "        cell_id = parts[1]\n",
    "        color = parts[2]\n",
    "        # Store in dictionary\n",
    "        cell_color_dict[cell_id] = color\n",
    "\n",
    "# Define the path to the tensor sample order file\n",
    "#This file contains the prefixes in the form\n",
    "#sc11.ACTTGA\n",
    "#sc11.CGATGT\n",
    "#sc11.GCCAAT\n",
    "#\n",
    "tensor_order_filename = f'{output_directory}/filtered_bam_list.txt'\n",
    "\n",
    "# Initialize a list to store the 1s and 0s\n",
    "color_vector = []\n",
    "\n",
    "# Open and read the tensor sample order file\n",
    "with open(tensor_order_filename, 'r') as file:\n",
    "    for line in file:\n",
    "        sample_id = line.strip()  # Remove any trailing newlines or spaces\n",
    "        if sample_id in cell_color_dict and cell_color_dict[sample_id] == 'red':\n",
    "            color_vector.append(1)\n",
    "        else:\n",
    "            color_vector.append(0)\n",
    "\n",
    "# Output the color vector to check\n",
    "print(len(color_vector))\n",
    "\n",
    "# Create a mapping dictionary\n",
    "color_mapping = {\n",
    "    'red': 'imr90',\n",
    "    'blue': 'gm12878'\n",
    "}\n",
    "\n",
    "# Update the dictionary using the mapping\n",
    "updated_cell_color_dict = {key: color_mapping[value] for key, value in cell_color_dict.items()}\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the desired rank\n",
    "rank = 2\n",
    "\n",
    "# Perform Non-negative Matrix Factorization (NMF)\n",
    "model = NMF(n_components=rank, init='random', random_state=0)\n",
    "W = model.fit_transform(methylation_matrix_1Mb)\n",
    "H = model.components_\n",
    "\n",
    "# Check the shapes of W and H to confirm the decomposition\n",
    "print(\"Shape of W:\", W.shape)\n",
    "print(\"Shape of H:\", H.shape)\n",
    "\n",
    "# Assuming W is already computed\n",
    "\n",
    "# Create a color vector based on the index values\n",
    "indices = np.arange(W.shape[0])\n",
    "color_vector_W = indices  # Using indices as colors\n",
    "\n",
    "# Normalize the color vector to range [0, 1] for use in colormap\n",
    "color_vector_W_normalized = (color_vector_W - color_vector_W.min()) / (color_vector_W.max() - color_vector_W.min())\n",
    "\n",
    "# Scatter plot for W factors with coloring by index\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(W[:, 0], W[:, 1], c=color_vector_W_normalized, cmap='viridis', alpha=0.7)\n",
    "plt.title('Scatter Plot of W Factors')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Index')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Scatter plot for H factors\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(H[0, :], H[1, :], c = color_vector, alpha=0.7)\n",
    "plt.title('Scatter Plot of H Factors')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7481b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiomics6",
   "language": "python",
   "name": "multiomics6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
