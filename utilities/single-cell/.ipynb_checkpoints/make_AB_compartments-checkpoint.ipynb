{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a79fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# imports\n",
    "############################################\n",
    "\n",
    "import pyBigWig\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from heapq import nlargest\n",
    "import copy\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from config_and_print import methy_directory, filtered_list, chrom_file, resolutions, output_directory, mappability_threshold\n",
    "#chromosomes = [f'chr{chrom}' for chrom in chromosomes]\n",
    "\n",
    "# Ensure resolutions is treated as a tuple or list of strings\n",
    "if isinstance(resolutions, str):\n",
    "    resolutions = (resolutions,)\n",
    "\n",
    "# Print resolutions for debugging\n",
    "print(f\"Resolutions from config: {resolutions}\")\n",
    "\n",
    "# Extract resolution value and label from the resolutions string\n",
    "resolution_str = resolutions[0]\n",
    "\n",
    "# Debug print to check the value of resolution_str\n",
    "print(f\"Extracted resolution string: {resolution_str}\")\n",
    "\n",
    "def parse_resolution(resolution_str):\n",
    "    if ':' in resolution_str:\n",
    "        resolution_value, resolution_label = resolution_str.split(':')\n",
    "        try:\n",
    "            resolution = int(resolution_value)\n",
    "            return resolution, resolution_label\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Resolution value should be an integer: '{resolution_value}' in '{resolution_str}'\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid resolution format: '{resolution_str}'. Expected format 'value:label', e.g., '1000000:1Mb'.\")\n",
    "\n",
    "resolution, resolution_label = parse_resolution(resolution_str)\n",
    "\n",
    "########################################################################\n",
    "# create the cell type dictionary\n",
    "# [TO DO] This needs to be replaced with SNPS code \n",
    "########################################################################\n",
    "# Define the path file with prefixes and colors in the following form\n",
    "#1       sc1.ACTTGA      red\n",
    "#2       sc1.GCCAAT      red\n",
    "#3       sc1.TAGCTT      red\n",
    "#4       sc10.TAGCTT     blue\n",
    "#\n",
    "filename = '../../bin/name.order.HCG_methy.with_color.txt'\n",
    "\n",
    "# Initialize an empty dictionary to store cell ID and color\n",
    "cell_color_dict = {}\n",
    "\n",
    "# Open and read the file\n",
    "with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into parts\n",
    "        parts = line.strip().split()\n",
    "        # Extract cell ID and color\n",
    "        cell_id = parts[1]\n",
    "        color = parts[2]\n",
    "        # Store in dictionary\n",
    "        cell_color_dict[cell_id] = color\n",
    "\n",
    "# Define the path to the tensor sample order file\n",
    "#This file contains the prefixes in the form\n",
    "#sc11.ACTTGA\n",
    "#sc11.CGATGT\n",
    "#sc11.GCCAAT\n",
    "#\n",
    "tensor_order_filename = f'{output_directory}/filtered_bam_list.txt'\n",
    "\n",
    "# Initialize a list to store the 1s and 0s\n",
    "color_vector = []\n",
    "\n",
    "# Open and read the tensor sample order file\n",
    "with open(tensor_order_filename, 'r') as file:\n",
    "    for line in file:\n",
    "        sample_id = line.strip()  # Remove any trailing newlines or spaces\n",
    "        if sample_id in cell_color_dict and cell_color_dict[sample_id] == 'red':\n",
    "            color_vector.append(1)\n",
    "        else:\n",
    "            color_vector.append(0)\n",
    "\n",
    "# Output the color vector to check\n",
    "print(len(color_vector))\n",
    "\n",
    "# Create a mapping dictionary\n",
    "color_mapping = {\n",
    "    'red': 'imr90',\n",
    "    'blue': 'gm12878'\n",
    "}\n",
    "\n",
    "# Update the dictionary using the mapping\n",
    "updated_cell_color_dict = {key: color_mapping[value] for key, value in cell_color_dict.items()}\n",
    "\n",
    "#################################################################################\n",
    "#create dark bins file if not already created\n",
    "#################################################################################\n",
    "\n",
    "# Check if the bins to remove file has already been created\n",
    "bins_file_path = f'{output_directory}/bins_to_remove_res{resolution_label}.npz'\n",
    "if os.path.exists(bins_file_path):\n",
    "    print(f\"{bins_file_path} already exists. Skipping computation.\")\n",
    "    #load the dark regions data and the A/B compartment data\n",
    "    loaded_data = np.load(bins_file_path)\n",
    "    # Convert the loaded data back into a dictionary with the same structure\n",
    "    bins_to_remove = {chrom: loaded_data[chrom] for chrom in loaded_data}\n",
    "else:\n",
    "    bigwig_file = \"../../bin/softwarefiles/dark_regions_hg19.bigWig\"\n",
    "    # Open the BigWig file\n",
    "    bw = pyBigWig.open(bigwig_file)\n",
    "\n",
    "    # Define the chromosomes you want to analyze\n",
    "    chromosomes = ['chr' + str(i) for i in range(1, 23)] \n",
    "\n",
    "    # Define the threshold for removing bins based on average mappability\n",
    "    threshold = mappability_threshold\n",
    "\n",
    "    # Create a dictionary to store the bin indices to remove for each chromosome\n",
    "    bins_to_remove = {}\n",
    "\n",
    "    # Loop through each chromosome\n",
    "    for chrom in chromosomes:\n",
    "        chrom_size = bw.chroms(chrom)\n",
    "\n",
    "        if chrom_size is None:\n",
    "            print(f\"Chromosome {chrom} not found in the BigWig file.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the number of bins based on the specified resolution\n",
    "        num_bins = math.ceil(chrom_size / resolution) #last bin may not be of size resolution\n",
    "\n",
    "        # Create lists to store bin indices to remove\n",
    "        remove_indices = []\n",
    "\n",
    "        # Calculate average mappability for each bin\n",
    "        for i in range(num_bins):\n",
    "            # Determine the start and end positions of the bin\n",
    "            start = i * resolution\n",
    "            end = min((i + 1) * resolution, chrom_size)  # to account for last bin which may be incomplete\n",
    "\n",
    "            # Extract the mappability values for the bin\n",
    "            values = np.nan_to_num(bw.values(chrom, start, end))\n",
    "\n",
    "            # Calculate the average mappability score for the bin\n",
    "            avg_mappability = np.mean(values)\n",
    "\n",
    "            # Check if the average mappability is below the threshold\n",
    "            if avg_mappability < threshold:\n",
    "                remove_indices.append(i)\n",
    "\n",
    "        # Store the bin indices to remove for this chromosome\n",
    "        bins_to_remove[chrom] = remove_indices\n",
    "\n",
    "    # Close the BigWig file\n",
    "    bw.close()\n",
    "\n",
    "    # Convert the lists in bins_to_remove to numpy arrays\n",
    "    for chrom in bins_to_remove:\n",
    "        bins_to_remove[chrom] = np.array(bins_to_remove[chrom])\n",
    "\n",
    "    # Save the dictionary as an .npz file\n",
    "    np.savez(bins_file_path, **bins_to_remove)\n",
    "    print(f\"Bins to remove file created and saved to {bins_file_path}\")\n",
    "\n",
    "###########################################################################    \n",
    "#create a dictionary of the A/B compartment calls for the bulk data\n",
    "###########################################################################\n",
    "bulk_data = {}\n",
    "path_to_eigenvectors = '../../projects/single_cell_files/eigenvector/'\n",
    "\n",
    "for i in range(1, 23):\n",
    "    file = path_to_eigenvectors + f'res{resolution}_ch{i}_oe_GM12878_KR_eigenvector.txt'\n",
    "    key = os.path.splitext(os.path.basename(file))[0]  \n",
    "    bulk_data[key] = pd.read_csv(file, header=None, names=['eigenvalue'])\n",
    "    file = path_to_eigenvectors + f'res{resolution}_ch{i}_oe_IMR90_KR_eigenvector.txt'\n",
    "    key = os.path.splitext(os.path.basename(file))[0]  \n",
    "    bulk_data[key] = pd.read_csv(file, header=None, names=['eigenvalue'])\n",
    "    \n",
    "###########################################################################\n",
    "# download H3K9ac file if it does not exist\n",
    "###########################################################################\n",
    "\n",
    "def calculate_bin_averages(data, elements_per_bin):\n",
    "    num_bins = math.ceil(len(data)/elements_per_bin)\n",
    "    bin_averages = np.zeros(num_bins)\n",
    "    for i in range(num_bins):\n",
    "        start_index = i * elements_per_bin\n",
    "        end_index = min((i + 1) * elements_per_bin, len(data))\n",
    "        bin_data = data[start_index:end_index]\n",
    "        if len(bin_data) > 0:\n",
    "            bin_averages[i] = np.mean(bin_data)\n",
    "        else:\n",
    "            bin_averages[i] = 0\n",
    "    return np.nan_to_num(bin_averages, nan=0.0)\n",
    "\n",
    "# Read chromosome sizes from hg19.autosome.chrom.sizes\n",
    "lengths = {}\n",
    "with open(chrom_file, 'r') as file:\n",
    "    for line in file:\n",
    "        chrom, size = line.strip().split()\n",
    "        lengths[chrom] = int(size)\n",
    "\n",
    "# Check if the output file already exists\n",
    "output_file = f'../../bin/softwarefiles/h3k9ac_res{resolution}_GM12878.pkl'\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    # Initialization\n",
    "    chromosomes = list(lengths.keys())\n",
    "    h3k9ac = {name: [] for name in chromosomes}\n",
    "\n",
    "    bigwig_H3K9ac_path = '../../bin/softwarefiles/ENCFF128UVW_hg19_H3K9ac_GM12878.bigWig'\n",
    "    if not os.path.exists(bigwig_H3K9ac_path):\n",
    "        os.system(f'wget https://www.encodeproject.org/files/ENCFF128UVW/@@download/ENCFF128UVW.bigWig -O {bigwig_H3K9ac_path}')\n",
    "\n",
    "    bigwig_H3K9ac = pyBigWig.open(bigwig_H3K9ac_path)\n",
    "    \n",
    "    # Process each chromosome\n",
    "    for chromosome_name in chromosomes:\n",
    "        start_position = 1\n",
    "        end_position = lengths[chromosome_name]\n",
    "        values_file1 = np.array(bigwig_H3K9ac.values(chromosome_name, start_position, end_position))\n",
    "        h3k9ac[chromosome_name] = calculate_bin_averages(values_file1, resolution)\n",
    "\n",
    "    # Save the results in a pickle file\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(h3k9ac, file)\n",
    "else:\n",
    "    print(f\"The file {output_file} already exists.\")\n",
    "    with open(output_file, 'rb') as file:\n",
    "        h3k9ac = pickle.load(file)\n",
    "\n",
    "################################################################################    \n",
    "#make sure each GM12878 eigenvector has positive value for active A compartment\n",
    "################################################################################\n",
    "for i in range(1, 23):\n",
    "    key_gm12878 = f'res{resolution}_ch{i}_oe_GM12878_KR_eigenvector'\n",
    "    chromosome_key = f'chr{i}'\n",
    "    h3k9ac_df = pd.DataFrame(h3k9ac[chromosome_key], columns=['H3K9ac_signal'])\n",
    "    \n",
    "    df_gm12878_positive = bulk_data[key_gm12878]['eigenvalue']  \n",
    "    df_gm12878_negative = -bulk_data[key_gm12878]['eigenvalue']\n",
    "\n",
    "    # Compute correlations by first ensuring eigenvector data is in DataFrame format\n",
    "    corr_positive = df_gm12878_positive.corr(h3k9ac_df['H3K9ac_signal'])\n",
    "    corr_negative = df_gm12878_negative.corr(h3k9ac_df['H3K9ac_signal'])\n",
    "    if corr_negative > corr_positive:\n",
    "        bulk_data[key_gm12878]['eigenvalue'] = -bulk_data[key_gm12878]['eigenvalue']\n",
    "        print(f\"Switched for chromosome {i}\")    \n",
    "\n",
    "###############################################################################    \n",
    "#make sure each IMR90 eigenvector has consistent orientation with GM12878\n",
    "################################################################################\n",
    "for i in range(1, 23):\n",
    "    # Construct keys for GM12878 and IMR90\n",
    "    key_gm12878 = f'res{resolution}_ch{i}_oe_GM12878_KR_eigenvector'\n",
    "    key_imr90 = f'res{resolution}_ch{i}_oe_GM12878_KR_eigenvector'\n",
    "    \n",
    "    # Retrieve DataFrames for GM12878 and IMR90\n",
    "    df_gm12878 = bulk_data[key_gm12878]\n",
    "    df_imr90 = bulk_data[key_imr90]\n",
    "    \n",
    "    # Ensure data is in expected format\n",
    "    if not df_gm12878.empty and not df_imr90.empty:\n",
    "        # Concatenate DataFrames side-by-side\n",
    "        combined_df = pd.concat([df_gm12878.reset_index(drop=True), df_imr90.reset_index(drop=True)], axis=1, keys=['gm12878', 'imr90'])\n",
    "        \n",
    "        # Drop rows with NaN values in either column\n",
    "        combined_df.dropna(inplace=True)\n",
    "        \n",
    "        # Extract Series after dropping NaNs\n",
    "        gm12878_series = combined_df['gm12878']['eigenvalue']\n",
    "        imr90_series = combined_df['imr90']['eigenvalue']\n",
    "\n",
    "        # Calculate correlations\n",
    "        corr_positive = gm12878_series.corr(imr90_series)\n",
    "        corr_negative = gm12878_series.corr(-imr90_series)\n",
    "\n",
    "        # If negating IMR90 improves correlation, update the original data\n",
    "        if corr_negative > corr_positive:\n",
    "            bulk_data[key_imr90]['eigenvalue'] = -bulk_data[key_imr90]['eigenvalue']\n",
    "            print(f\"switched for chromosome {i}\")\n",
    "\n",
    "#The A/B compartments of the proper orientation before dropping dark regions\n",
    "original_bulk_data = copy.deepcopy(bulk_data)\n",
    "\n",
    "#################################################################################\n",
    "#remove dark regions\n",
    "#dark reigons are obviously correlated\n",
    "#I want to remove dark regions to get meeaningfully correlated regions\n",
    "###############################################################################\n",
    "\n",
    "for i in range(1, 23):\n",
    "    # Construct the keys for GM12878 and IMR90\n",
    "    key_gm12878 = f'res{resolution}_ch{i}_oe_GM12878_KR_eigenvector'\n",
    "    key_imr90 = f'res{resolution}_ch{i}_oe_IMR90_KR_eigenvector'\n",
    "    chrom = f'chr{i}'\n",
    "\n",
    "    # Check if the chromosome exists in the bins_to_remove and in the data\n",
    "    if chrom in bins_to_remove and key_gm12878 in bulk_data and key_imr90 in bulk_data:\n",
    "        # Retrieve the indices to remove for this chromosome\n",
    "        indices_to_remove = bins_to_remove[chrom]\n",
    "\n",
    "        # Initialize lists to hold valid indices for both DataFrames\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx in indices_to_remove:\n",
    "            if idx < len(bulk_data[key_gm12878]) and idx < len(bulk_data[key_imr90]):\n",
    "                valid_indices.append(idx)\n",
    "        \n",
    "        # Now drop the valid indices from both DataFrames\n",
    "        if valid_indices:\n",
    "            bulk_data[key_gm12878] = bulk_data[key_gm12878].drop(valid_indices).reset_index(drop=True)\n",
    "            bulk_data[key_imr90] = bulk_data[key_imr90].drop(valid_indices).reset_index(drop=True)\n",
    "\n",
    "########################################################################\n",
    "#compute the genome-wide correlation of the two AB compartment cell types\n",
    "# should be somehting like .6 if all the above steps were done correctly\n",
    "# this is a sanity check\n",
    "# There seems to be a problem here\n",
    "########################################################################\n",
    "# Initialize lists to collect the eigenvector values for IMR90 and GM12878\n",
    "imr90_values = []\n",
    "gm12878_values = []\n",
    "\n",
    "# Loop through each chromosome\n",
    "for i in range(1, 23):\n",
    "    key_gm12878 = f'res{resolution}_ch{i}_oe_GM12878_KR_eigenvector'\n",
    "    key_imr90 = f'res{resolution}_ch{i}_oe_IMR90_KR_eigenvector'                                                               \n",
    "                                                               \n",
    "    # Check if the keys exist in the data to avoid KeyErrors\n",
    "    if key_gm12878 in bulk_data and key_imr90 in bulk_data:\n",
    "        # Extend the list with the eigenvector values from each chromosome\n",
    "        # Ensure values are numeric (floats or ints)\n",
    "        gm12878_vals = bulk_data[key_gm12878]['eigenvalue']\n",
    "        imr90_vals = bulk_data[key_imr90]['eigenvalue']\n",
    "        imr90_values.extend(imr90_vals)\n",
    "        gm12878_values.extend(gm12878_vals)\n",
    "        \n",
    "# Convert lists to pandas Series for correlation calculation\n",
    "imr90_series = pd.Series(imr90_values)\n",
    "gm12878_series = pd.Series(gm12878_values)\n",
    "\n",
    "# Calculate the Pearson correlation coefficient between IMR90 and GM12878\n",
    "correlation = imr90_series.corr(gm12878_series)\n",
    "\n",
    "print(f'Genome-wide correlation between IMR90 and GM12878: {correlation}')\n",
    "\n",
    "                                                               \n",
    "                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec45813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90585725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def extract_prefixes(file_path):\n",
    "    \"\"\"Extract prefixes from the filtered_bam_list.txt file.\"\"\"\n",
    "    prefixes = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Assuming the prefix format is like \"sc11.ACTTGA\" or similar\n",
    "            parts = line.strip().split('.')\n",
    "            if len(parts) >= 2:\n",
    "                prefix = parts[0] + '.' + parts[1]  # Concatenate the first two parts to get the full prefix\n",
    "                prefixes.append(prefix)\n",
    "    return prefixes\n",
    "\n",
    "def load_h5_file(file_path, dataset_name):\n",
    "    \"\"\"Load a dataset from an HDF5 file.\"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        data = f[dataset_name][:]\n",
    "    return data\n",
    "\n",
    "def get_best_correlated_vector(V, eigenvector):\n",
    "    \"\"\"Find the row in V that best correlates with the given eigenvector.\"\"\"\n",
    "    eigenvector = eigenvector.values.flatten()\n",
    "    best_corr = -np.inf  # Initialize with a very low correlation\n",
    "    best_index = -1\n",
    "    best_vector = None\n",
    "\n",
    "    # Check each row in V for correlation with the eigenvector\n",
    "    for i in range(V.shape[0]):  # Iterate over all rows in V\n",
    "        # Synchronize non-NaN data\n",
    "        valid_indices = ~np.isnan(V[i, :]) & ~np.isnan(eigenvector)\n",
    "        if np.any(valid_indices):\n",
    "            corr, _ = pearsonr(V[i, valid_indices], eigenvector[valid_indices])\n",
    "            if corr > best_corr:  # Check if this is the best correlation so far\n",
    "                best_corr = corr\n",
    "                best_index = i\n",
    "                best_vector = V[i, :]\n",
    "\n",
    "    return best_vector, best_index, best_corr\n",
    "\n",
    "# Directory setup\n",
    "output_directory = '../../projects/single_cell_files/'\n",
    "filtered_bam_list = os.path.join(output_directory, 'filtered_bam_list.txt')\n",
    "base_tensor_dir = os.path.join(output_directory, 'tensor_1Mb_AB_factors')\n",
    "\n",
    "# Extract prefixes from the file\n",
    "prefixes = extract_prefixes(filtered_bam_list)\n",
    "print(f\"Extracted prefixes: {prefixes}\")\n",
    "\n",
    "# Dictionary to hold the dataframes for each chromosome\n",
    "chromosome_results = {}\n",
    "\n",
    "def normalize_vectors(V):\n",
    "    \"\"\"Normalize the columns of V to have a norm of 1.\"\"\"\n",
    "    norms = np.linalg.norm(V, axis=0)\n",
    "    norms[norms == 0] = 1  # Prevent division by zero\n",
    "    V_normalized = V / norms\n",
    "    return .02 * V_normalized - .001\n",
    "\n",
    "def normalize_vectors_gradual(V, min_val=-0.05, max_val=0.05):\n",
    "    \"\"\"Normalize the columns of V with a more gradual change between entries.\"\"\"\n",
    "    norms = np.linalg.norm(V, axis=0)\n",
    "    norms[norms == 0] = 1  # Prevent division by zero\n",
    "    V_normalized = V / norms\n",
    "    \n",
    "    # Apply a sigmoid function for a more gradual change\n",
    "    V_sigmoid = 1 / (1 + np.exp(-V_normalized))  # Compresses values into (0, 1)\n",
    "    \n",
    "    # Rescale to the desired range\n",
    "    V_rescaled = min_val + (V_sigmoid - V_sigmoid.min()) / (V_sigmoid.max() - V_sigmoid.min()) * (max_val - min_val)\n",
    "    \n",
    "    return V_rescaled\n",
    "\n",
    "def threshold_and_normalize_vectors(V):\n",
    "    \"\"\"Threshold each vector so the top 5% of values are set to the minimum of those top 5%, then normalize to norm 1.\"\"\"\n",
    "    V_thresholded = np.copy(V)\n",
    "    num_elements = V.shape[0]\n",
    "    \n",
    "    for i in range(V.shape[1]):\n",
    "        # Determine the threshold for the top 5%\n",
    "        threshold_index = max(1, int(num_elements * 0.05))\n",
    "        sorted_indices = np.argsort(V[:, i])\n",
    "        top_indices = sorted_indices[-threshold_index:]\n",
    "        threshold_value = V[top_indices[0], i]\n",
    "        \n",
    "        # Apply the threshold\n",
    "        V_thresholded[top_indices, i] = threshold_value\n",
    "    \n",
    "    # Normalize each column to have a norm of 1\n",
    "    norms = np.linalg.norm(V_thresholded, axis=0)\n",
    "    norms[norms == 0] = 1  # Prevent division by zero\n",
    "    V_normalized = V_thresholded / norms\n",
    "    \n",
    "    return V_normalized\n",
    "\n",
    "def normalize_vectors_log(V, min_val=-0.05, max_val=0.05):\n",
    "    \"\"\"Normalize the columns of V with a more gradual change using a logarithmic transformation.\"\"\"\n",
    "    norms = np.linalg.norm(V, axis=0)\n",
    "    norms[norms == 0] = 1  # Prevent division by zero\n",
    "    V_normalized = V / norms\n",
    "    \n",
    "    # Shift values to be strictly positive before applying log (assuming V is non-negative)\n",
    "    V_shifted = V_normalized + np.finfo(float).eps  # Add a small epsilon to avoid log(0)\n",
    "    \n",
    "    # Apply logarithmic transformation\n",
    "    V_log = np.log(V_shifted)\n",
    "    \n",
    "    # Rescale to the desired range\n",
    "    V_min_log = np.min(V_log, axis=0)\n",
    "    V_max_log = np.max(V_log, axis=0)\n",
    "    \n",
    "    V_rescaled = min_val + (V_log - V_min_log) / (V_max_log - V_min_log + np.finfo(float).eps) * (max_val - min_val)\n",
    "    \n",
    "    return V_rescaled\n",
    "\n",
    "\n",
    "def normalize_vectors_to_range(V, min_val=-0.05, max_val=0.05):\n",
    "    \"\"\"Normalize the columns of V to have values between min_val and max_val.\"\"\"\n",
    "    V_min = np.min(V, axis=0)\n",
    "    V_max = np.max(V, axis=0)\n",
    "    \n",
    "    # Avoid division by zero if V_max equals V_min\n",
    "    scale = (max_val - min_val) / (V_max - V_min + np.finfo(float).eps)\n",
    "    \n",
    "    V_normalized = min_val + (V - V_min) * scale\n",
    "    return V_normalized\n",
    "\n",
    "\n",
    "# Loop through each chromosome directory\n",
    "for i in range(1, 23):\n",
    "    chromosome = f'chr{i}'\n",
    "    results_df = pd.DataFrame(columns=['Sample', 'A/B Compartment', 'Correlation With Bulk', 'Cell Type'])\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        input_file = os.path.join(base_tensor_dir, chromosome, f'{prefix}_compartments.h5')\n",
    "        output_dir = os.path.join(output_directory, 'tensor_1Mb_AB_calls', chromosome)\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        output_file = os.path.join(output_dir, f'{prefix}_tensor_AB_compartment_call.h5')\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"{output_file} already exists. Loading existing results.\")\n",
    "            with h5py.File(output_file, 'r') as output_h5:\n",
    "                best_vector = output_h5['AB_Compartment'][:]\n",
    "                best_corr = output_h5['correlation']\n",
    "                cell_type = output_h5['cell_type']\n",
    " \n",
    "        else:\n",
    "            sample_id = prefix  \n",
    "            cell_type = updated_cell_color_dict.get(sample_id, 'GM12878')  # Default set to GM12878\n",
    "            key = f'res{resolution}_ch{i}_oe_{cell_type.upper()}_KR_eigenvector'  \n",
    "\n",
    "            print(f\"Processing sample_id: {sample_id}, cell_type: {cell_type}, key: {key}, resolution {resolution}\")\n",
    "\n",
    "            if key in original_bulk_data:\n",
    "                tensor_factors = load_h5_file(input_file, '/compartment_factors')\n",
    "                tensor_factors = normalize_vectors(tensor_factors)\n",
    "                print(f\"Loaded tensor factors from {input_file}, shape: {tensor_factors.shape}\")\n",
    "                bulk_eigenvector = original_bulk_data[key]['eigenvalue'][:-1]\n",
    "                print(f\"Bulk eigenvector shape: {bulk_eigenvector.shape}\")\n",
    "\n",
    "                best_vector, best_index, best_corr = get_best_correlated_vector(tensor_factors, bulk_eigenvector)\n",
    "                print(f\"Best correlation: {best_corr} at index {best_index}\")\n",
    "\n",
    "                # Save the results to the output file\n",
    "                with h5py.File(output_file, 'w') as output_h5:\n",
    "                    output_h5.create_dataset('AB_Compartment', data=best_vector)\n",
    "                    output_h5.create_dataset('correlation', data=best_corr)\n",
    "                    output_h5.create_dataset('cell_type', data=cell_type)\n",
    "\n",
    "        new_row = pd.DataFrame({\n",
    "            'Sample': [sample_id],\n",
    "            'A/B Compartment': [best_vector],\n",
    "            'Correlation With Bulk': [best_corr],\n",
    "            'Cell Type': [cell_type]\n",
    "        })\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    print(f\"Finished processing {chromosome}, {results_df.shape[0]} rows added.\")\n",
    "    chromosome_results[chromosome] = results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d075bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosome_results['chr1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588240c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf41cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'sc19.TAGCTT'\n",
    "output_directory = '../../projects/single_cell_files/'\n",
    "output_dir = os.path.join(output_directory, 'tensor_1Mb_AB_calls', 'chr1')\n",
    "output_file = os.path.join(output_dir, f'{prefix}_tensor_AB_compartment_call.h5')\n",
    "\n",
    "with h5py.File(output_file, 'r') as output_h5:\n",
    "    best_vector = output_h5['AB_Compartment'][:]\n",
    "    best_corr = output_h5['correlation']\n",
    "    cell_type = output_h5['cell_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ea246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chr=1    \n",
    "# Assuming chromosome_results['chr1'] contains relevant data\n",
    "sc_dataframe = chromosome_results[f'chr{chr}']\n",
    "\n",
    "# First, sort by 'Cell Type' in a way that 'gm12878' cells come first, and then by 'Correlation With Bulk' descending\n",
    "sc_dataframe['Cell Type Sort'] = sc_dataframe['Cell Type'].map({'gm12878': 0, 'imr90': 1})  # Helps in sorting by cell type\n",
    "sc_dataframe = sc_dataframe.sort_values(by=['Cell Type Sort', 'Correlation With Bulk'], ascending=[True, False])\n",
    "\n",
    "# Remove the auxiliary column after sorting\n",
    "sc_dataframe.drop('Cell Type Sort', axis=1, inplace=True)\n",
    "\n",
    "# Now sc_dataframe is sorted first by cell type ('gm12878' first) and then by 'Correlation With Bulk' within each type\n",
    "sc_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm12878_count = sc_dataframe[sc_dataframe['Cell Type'] == 'gm12878'].shape[0]\n",
    "gm12878_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90531bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "def plot_eigenvectors_with_heatmap(bulk_eigenvector1, bulk_eigenvector2, sc_dataframe, title1, title2, title, save_path=None):\n",
    "    fig = plt.figure(figsize=(15, 20))\n",
    "    gs = gridspec.GridSpec(nrows=3, ncols=2, figure=fig, width_ratios=[30, 1], height_ratios=[1, 1, 4], hspace=0.3, wspace=0.05)\n",
    "\n",
    "    # Plot Bulk Eigenvectors\n",
    "    ax_bulk1 = fig.add_subplot(gs[0, 0])\n",
    "    ax_bulk1.plot(bulk_eigenvector1, color='blue')\n",
    "    ax_bulk1.set_title(title1, fontsize=15)\n",
    "    ax_bulk1.set_xlabel(\"Bin Index\", fontsize=15)\n",
    "    ax_bulk1.set_ylabel(\"Eigenvector Value\", fontsize=15)\n",
    "\n",
    "    ax_bulk2 = fig.add_subplot(gs[1, 0])\n",
    "    ax_bulk2.plot(bulk_eigenvector2, color='red')\n",
    "    ax_bulk2.set_title(title2, fontsize=15)\n",
    "    ax_bulk2.set_xlabel(\"Bin Index\", fontsize=15)\n",
    "    ax_bulk2.set_ylabel(\"Eigenvector Value\", fontsize=15)\n",
    "\n",
    "    # Create heatmap from 'A Compartment' column\n",
    "    ax_sc = fig.add_subplot(gs[2, 0])\n",
    "    # Extract all arrays from 'A Compartment' and stack them\n",
    "    sc_data_array = np.stack(sc_dataframe['A/B Compartment'].apply(lambda x: np.array(x)).tolist())\n",
    "    im = ax_sc.imshow(sc_data_array, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "    ax_sc.set_title(\"Heatmap of Single Cell Samples\", fontsize=15)\n",
    "    ax_sc.set_xlabel(\"Bin Index\", fontsize=15)\n",
    "    ax_sc.set_ylabel(\"Sample Index\", fontsize=15)\n",
    "\n",
    "    # Colorbar for the heatmap\n",
    "    ax_cbar = fig.add_subplot(gs[2, 1])\n",
    "    fig.colorbar(im, cax=ax_cbar)\n",
    "    ax_cbar.set_ylabel('Compartment Call Value', rotation=270, labelpad=15, fontsize=15)\n",
    "\n",
    "    plt.suptitle(title, fontsize=30)\n",
    "\n",
    "    # Save the figure if a save path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example bulk eigenvector data, replace with your actual data\n",
    "key_gm12878 = f'res{resolution}_ch{chr}_oe_GM12878_KR_eigenvector'\n",
    "key_imr90 = f'res{resolution}_ch{chr}_oe_IMR90_KR_eigenvector'    \n",
    "\n",
    "bulk_eigenvector_gm12878 = original_bulk_data[key_gm12878]['eigenvalue']\n",
    "bulk_eigenvector_imr90 = original_bulk_data[key_imr90]['eigenvalue']\n",
    "\n",
    "plot_eigenvectors_with_heatmap(\n",
    "    bulk_eigenvector_gm12878,\n",
    "    bulk_eigenvector_imr90,\n",
    "    sc_dataframe,\n",
    "    \"Bulk for chr1 - GM12878\",\n",
    "    \"Bulk for chr1 - IMR90\",\n",
    "    \"Single Cell Compartment Call Comparison\",\n",
    "    save_path=\"../../files/AB_compartment_heatmap.png\"  # Specify the file path to save the image\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df721fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edc827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e9088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiomics6",
   "language": "python",
   "name": "multiomics6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
