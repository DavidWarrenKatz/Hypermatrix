{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ebe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hicstraw\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.stats import pearsonr\n",
    "import h5py\n",
    "import hicstraw\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.stats import pearsonr\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.io\n",
    "from sklearn.manifold import TSNE\n",
    "import gzip\n",
    "from sklearn.decomposition import NMF\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import non_negative_parafac\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as stats\n",
    "from numpy.linalg import norm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import warnings\n",
    "from scipy.stats import zscore\n",
    "from scipy.interpolate import interp1d, interp2d\n",
    "import os\n",
    "import sys\n",
    "from scipy.sparse import coo_matrix, csr_matrix, triu, tril\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "base_directory = \"/home/dwk681/workspace/multi_omics_hic_clustering/scNOMeHiC_20210127/250Kb/methylation_files/\"\n",
    "file_path = base_directory + 'b37.autosome.250kb_interval.add_value.methy.GM_IMR90_128_samples.bed.gz'\n",
    "\n",
    "def normalize_matrix_columns(A):\n",
    "    \"\"\"\n",
    "    Normalize each column of the matrix so that each has a norm of one.\n",
    "    \n",
    "    Parameters:\n",
    "    - A: a 2D NumPy array (matrix)\n",
    "    \n",
    "    Returns:\n",
    "    - normalized_A: a matrix where each column of A has been divided by its L2 norm\n",
    "    \"\"\"\n",
    "    # Calculate the L2 norm for each column\n",
    "    column_norms = np.linalg.norm(A, axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if np.any(column_norms == 0):\n",
    "        raise ValueError(\"One or more columns have zero norm. Cannot normalize those columns.\")\n",
    "    \n",
    "    # Normalize each column by its norm\n",
    "    normalized_A = A / column_norms\n",
    "    \n",
    "    return normalized_A\n",
    "\n",
    "def calculate_matrix(file_path):\n",
    "    # Initialize an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Read the compressed file using gzip\n",
    "    with gzip.open(file_path, 'rt') as file:\n",
    "        for line in file:\n",
    "            # Split the line into fields\n",
    "            fields = line.strip().split('\\t')\n",
    "\n",
    "            # Extract relevant entries for calculations\n",
    "            numerator_indices = range(6, len(fields), 2)\n",
    "            denominator_indices = range(7, len(fields), 2)\n",
    "\n",
    "            row_data = []\n",
    "            for num_idx, denom_idx in zip(numerator_indices, denominator_indices):\n",
    "                numerator = float(fields[num_idx])\n",
    "                denominator = float(fields[denom_idx])\n",
    "\n",
    "                # Check if denominator is zero, set entry to 0, else perform the division\n",
    "                row_data.append(0 if denominator == 0 else numerator / denominator)\n",
    "\n",
    "            data.append(row_data)\n",
    "\n",
    "    # Convert the list of lists into a NumPy array\n",
    "    data_matrix = np.array(data)\n",
    "\n",
    "    return data_matrix\n",
    "\n",
    "methylation_matrix_1Mb = normalize_matrix_columns(calculate_matrix(file_path))\n",
    "print(methylation_matrix_1Mb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592a2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba12840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd86778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3b5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56da61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa502d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a46555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/dwk681/workspace/cluster_cells_from_GSE189158_NOMe_HiC/filesFromCluster/bam/methylation/filter_low_qual/sc24.ACTTGA.methy.b37.bw\n",
      "Processing /home/dwk681/workspace/cluster_cells_from_GSE189158_NOMe_HiC/filesFromCluster/bam/methylation/filter_low_qual/sc14.TAGCTT.methy.b37.bw\n",
      "Processing /home/dwk681/workspace/cluster_cells_from_GSE189158_NOMe_HiC/filesFromCluster/bam/methylation/filter_low_qual/sc17.CGATGT.methy.b37.bw\n",
      "Processing /home/dwk681/workspace/cluster_cells_from_GSE189158_NOMe_HiC/filesFromCluster/bam/methylation/filter_low_qual/sc19.ACTTGA.methy.b37.bw\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 118\u001b[0m\n\u001b[1;32m    116\u001b[0m     large_bin_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(res_value)\n\u001b[1;32m    117\u001b[0m     small_bin_size \u001b[38;5;241m=\u001b[39m large_bin_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m--> 118\u001b[0m     \u001b[43mprocess_bigwig_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbigwigfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlarge_bin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmall_bin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid resolution format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 62\u001b[0m, in \u001b[0;36mprocess_bigwig_file\u001b[0;34m(bigwigfile, large_bin_size, small_bin_size, resolution_label, base_directory, prefix)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Get data within the small bin\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     small_bin_data \u001b[38;5;241m=\u001b[39m \u001b[43mbw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchrom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msmall_bin_start\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msmall_bin_end\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Calculate the integral of data in the small bin\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     integral \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtrapz(small_bin_data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyBigWig\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "import h5py\n",
    "\n",
    "# Import variables from config_and_print.py\n",
    "from config_and_print import methy_directory, output_directory, resolutions, filtered_list\n",
    "\n",
    "# Read the filtered bam list\n",
    "with open(filtered_list, 'r') as f:\n",
    "    filtered_prefixes = set(line.strip() for line in f)\n",
    "\n",
    "# Function to process each bigWig file\n",
    "def process_bigwig_file(bigwigfile, large_bin_size, small_bin_size, resolution_label, base_directory, prefix):\n",
    "    bw = pyBigWig.open(bigwigfile)\n",
    "    chromosomes = bw.chroms()\n",
    "\n",
    "    for chrom in chromosomes:\n",
    "        # Ensure chrom_string is always in the format 'chrN'\n",
    "        if isinstance(chrom, int) or chrom.isdigit():\n",
    "            chrom_string = f'chr{chrom}'\n",
    "        else:\n",
    "            chrom_string = chrom if chrom.startswith('chr') else f'chr{chrom}'\n",
    "\n",
    "        if chrom_string in [f'chr{i}' for i in range(1, 23)]:\n",
    "            # Create chromosome-specific directory if it doesn't exist\n",
    "            chrom_directory = os.path.join(base_directory, chrom_string)\n",
    "            os.makedirs(chrom_directory, exist_ok=True)\n",
    "\n",
    "            # Define the filename for the matrix\n",
    "            new_filename = os.path.join(chrom_directory, f'{prefix}_{chrom_string}.h5')\n",
    "\n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(new_filename):\n",
    "                print(f\"File {new_filename} already exists, skipping computation.\")\n",
    "                continue\n",
    "\n",
    "            chrom_length = chromosomes[chrom]\n",
    "            large_bins = range(0, chrom_length, large_bin_size)\n",
    "            num_large_bins = len(large_bins)\n",
    "\n",
    "            integrals = np.zeros((large_bin_size // small_bin_size, num_large_bins))\n",
    "\n",
    "            for i, large_bin_start in enumerate(large_bins):\n",
    "                for j in range(large_bin_size // small_bin_size):\n",
    "                    small_bin_start = large_bin_start + j * small_bin_size\n",
    "                    small_bin_end = small_bin_start + small_bin_size\n",
    "\n",
    "                    # Ensure that the coordinates are within the chromosome boundaries\n",
    "                    if small_bin_start < 0:\n",
    "                        small_bin_start = 0\n",
    "                    if small_bin_end > chrom_length:\n",
    "                        small_bin_end = chrom_length\n",
    "\n",
    "                    # Check if the adjusted small_bin_start is still greater than or equal to small_bin_end\n",
    "                    if small_bin_start >= small_bin_end:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        # Get data within the small bin\n",
    "                        small_bin_data = bw.values(chrom, int(small_bin_start), int(small_bin_end))\n",
    "\n",
    "                        # Calculate the integral of data in the small bin\n",
    "                        integral = np.trapz(small_bin_data)\n",
    "\n",
    "                        # Store the integral in the corresponding list\n",
    "                        integrals[j, i] = integral\n",
    "                    except Exception as e:\n",
    "                        # Print the exception and relevant information\n",
    "                        print(f\"An error occurred for chromosome {chrom}, large bin {i}, small bin {j}: {e}\")\n",
    "\n",
    "            # create your matrices\n",
    "            num_rows, num_columns = integrals.shape\n",
    "            p_value_ks_matrix = np.zeros((num_columns, num_columns))\n",
    "            epsilon = 1e-10  # Define your epsilon value here\n",
    "\n",
    "            for i in range(num_columns):\n",
    "                for j in range(i, num_columns):\n",
    "                    if i == j:\n",
    "                        p_value_ks_matrix[i, j] = 1.0\n",
    "                    else:\n",
    "                        _, ks_p_value = ks_2samp(integrals[:, i], integrals[:, j])\n",
    "\n",
    "                        p_value_ks_matrix[i, j] = ks_p_value\n",
    "                        p_value_ks_matrix[j, i] = ks_p_value\n",
    "                        \n",
    "                        #p_value_ks_matrix[i, j] = np.log10(ks_p_value + epsilon)\n",
    "                        #p_value_ks_matrix[j, i] = np.log10(ks_p_value + epsilon)\n",
    "\n",
    "            # take the correlation matrices of all the matrices\n",
    "            correlation_ks_p_value = np.corrcoef(p_value_ks_matrix) + 1\n",
    "\n",
    "            # Save the correlation epigenetic matrix as a .h5 file\n",
    "            with h5py.File(new_filename, 'w') as hf:\n",
    "                hf.create_dataset('methy_matrix', data=p_value_ks_matrix)\n",
    "\n",
    "# Ensure resolutions is processed as a list of strings\n",
    "resolutions_list = [res.strip() for res in resolutions.split(',')]\n",
    "\n",
    "# Iterate through the files in methy_directory\n",
    "for root, dirs, files in os.walk(methy_directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.methy.b37.bw'):\n",
    "            # Extract the prefix for filtering\n",
    "            prefix = '.'.join(file.split('.')[:2])\n",
    "            if prefix in filtered_prefixes:\n",
    "                bigwigfile = os.path.join(root, file)\n",
    "                print(f\"Processing {bigwigfile}\")\n",
    "                for resolution in resolutions_list:\n",
    "                    try:\n",
    "                        res_value, res_label = resolution.split(\":\")\n",
    "                        # Create the base directory for the matrices\n",
    "                        base_directory = os.path.join(output_directory, f'methy_{res_label}_correlation_dir')\n",
    "                        os.makedirs(base_directory, exist_ok=True)\n",
    "                        large_bin_size = int(res_value)\n",
    "                        small_bin_size = large_bin_size // 100\n",
    "                        process_bigwig_file(bigwigfile, large_bin_size, small_bin_size, res_label, base_directory, prefix)\n",
    "                    except ValueError:\n",
    "                        print(f\"Invalid resolution format: {resolution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b7d11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1000000', '1Mb']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27448dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiomics6",
   "language": "python",
   "name": "multiomics6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
